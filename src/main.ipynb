{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Amann(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Amann is a neueral network for detecting amharic characters. \n",
    "        It does so by first resizing the image to 28x28 and then\n",
    "        applying multiple layers of convolution and pooling, finishing with\n",
    "        a linear layer.\n",
    "        \n",
    "        In Amharic there are 34 base characters each with 7 children. Thus the \n",
    "        end layer will have 34*7 = 238 outputs.\n",
    "        \n",
    "        One top of the neural network it has a regulairization layer to prevent\n",
    "        the small kernels from overfitting.\n",
    "        \"\"\"\n",
    "        super(Amann, self).__init__()\n",
    "        \n",
    "        \n",
    "        # convolution layers   \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=9, stride=1, padding=4)\n",
    "        \n",
    "        \n",
    "        # pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        \n",
    "        # linear layer\n",
    "        self.linear = nn.Linear(64*3*3, 34)\n",
    "        self.linear2 = nn.Linear(34, 238)\n",
    "        \n",
    "        # regularization layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolution layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # linear layers\n",
    "        x = x.view(-1, 64*3*3)\n",
    "        x = F.relu(self.linear(x))\n",
    "        #x = self.dropout(F.relu(self.linear(x)))\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmharicDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_filenames = os.listdir(data_dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.image_filenames[idx]\n",
    "        image = Image.open(os.path.join(self.data_dir, filename)).convert('L')\n",
    "        image = transforms.ToTensor()(image)\n",
    "        # Convert the image to grayscale using .convert('L')\n",
    "        label = int(filename.split('.')[0])\n",
    "        return image, label - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset, train the model, and save the model\n",
    "\n",
    "data_dir = \"../dataset/\"\n",
    "dataset = AmharicDataset(data_dir)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=80, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=80, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 5.475\n",
      "[1,   200] loss: 5.466\n",
      "[1,   300] loss: 5.440\n",
      "[2,   100] loss: 4.538\n",
      "[2,   200] loss: 3.395\n",
      "[2,   300] loss: 2.565\n",
      "[3,   100] loss: 1.769\n",
      "[3,   200] loss: 1.469\n",
      "[3,   300] loss: 1.332\n",
      "[4,   100] loss: 1.050\n",
      "[4,   200] loss: 0.959\n",
      "[4,   300] loss: 0.897\n",
      "[5,   100] loss: 0.728\n",
      "[5,   200] loss: 0.711\n",
      "[5,   300] loss: 0.670\n",
      "[6,   100] loss: 0.548\n",
      "[6,   200] loss: 0.586\n",
      "[6,   300] loss: 0.549\n",
      "[7,   100] loss: 0.423\n",
      "[7,   200] loss: 0.461\n",
      "[7,   300] loss: 0.474\n",
      "[8,   100] loss: 0.377\n",
      "[8,   200] loss: 0.371\n",
      "[8,   300] loss: 0.406\n",
      "[9,   100] loss: 0.301\n",
      "[9,   200] loss: 0.318\n",
      "[9,   300] loss: 0.349\n",
      "[10,   100] loss: 0.271\n",
      "[10,   200] loss: 0.273\n",
      "[10,   300] loss: 0.276\n",
      "[11,   100] loss: 0.214\n",
      "[11,   200] loss: 0.233\n",
      "[11,   300] loss: 0.243\n",
      "[12,   100] loss: 0.189\n",
      "[12,   200] loss: 0.209\n",
      "[12,   300] loss: 0.226\n",
      "[13,   100] loss: 0.158\n",
      "[13,   200] loss: 0.181\n",
      "[13,   300] loss: 0.175\n",
      "[14,   100] loss: 0.132\n",
      "[14,   200] loss: 0.132\n",
      "[14,   300] loss: 0.169\n",
      "[15,   100] loss: 0.117\n",
      "[15,   200] loss: 0.131\n",
      "[15,   300] loss: 0.136\n",
      "[16,   100] loss: 0.105\n",
      "[16,   200] loss: 0.102\n",
      "[16,   300] loss: 0.141\n",
      "[17,   100] loss: 0.087\n",
      "[17,   200] loss: 0.099\n",
      "[17,   300] loss: 0.106\n",
      "[18,   100] loss: 0.081\n",
      "[18,   200] loss: 0.081\n",
      "[18,   300] loss: 0.112\n",
      "[19,   100] loss: 0.064\n",
      "[19,   200] loss: 0.073\n",
      "[19,   300] loss: 0.095\n",
      "[20,   100] loss: 0.069\n",
      "[20,   200] loss: 0.066\n",
      "[20,   300] loss: 0.078\n",
      "[21,   100] loss: 0.068\n",
      "[21,   200] loss: 0.048\n",
      "[21,   300] loss: 0.063\n",
      "[22,   100] loss: 0.054\n",
      "[22,   200] loss: 0.055\n",
      "[22,   300] loss: 0.052\n",
      "[23,   100] loss: 0.038\n",
      "[23,   200] loss: 0.041\n",
      "[23,   300] loss: 0.065\n",
      "[24,   100] loss: 0.043\n",
      "[24,   200] loss: 0.035\n",
      "[24,   300] loss: 0.053\n",
      "[25,   100] loss: 0.046\n",
      "[25,   200] loss: 0.039\n",
      "[25,   300] loss: 0.056\n",
      "[26,   100] loss: 0.074\n",
      "[26,   200] loss: 0.048\n",
      "[26,   300] loss: 0.049\n",
      "[27,   100] loss: 0.037\n",
      "[27,   200] loss: 0.042\n",
      "[27,   300] loss: 0.044\n",
      "[28,   100] loss: 0.017\n",
      "[28,   200] loss: 0.017\n",
      "[28,   300] loss: 0.039\n",
      "[29,   100] loss: 0.045\n",
      "[29,   200] loss: 0.035\n",
      "[29,   300] loss: 0.038\n",
      "[30,   100] loss: 0.022\n",
      "[30,   200] loss: 0.033\n",
      "[30,   300] loss: 0.032\n",
      "[31,   100] loss: 0.022\n",
      "[31,   200] loss: 0.023\n",
      "[31,   300] loss: 0.029\n",
      "[32,   100] loss: 0.020\n",
      "[32,   200] loss: 0.022\n",
      "[32,   300] loss: 0.017\n",
      "[33,   100] loss: 0.017\n",
      "[33,   200] loss: 0.015\n",
      "[33,   300] loss: 0.014\n",
      "[34,   100] loss: 0.014\n",
      "[34,   200] loss: 0.016\n",
      "[34,   300] loss: 0.011\n",
      "[35,   100] loss: 0.025\n",
      "[35,   200] loss: 0.027\n",
      "[35,   300] loss: 0.026\n",
      "[36,   100] loss: 0.035\n",
      "[36,   200] loss: 0.039\n",
      "[36,   300] loss: 0.027\n",
      "[37,   100] loss: 0.025\n",
      "[37,   200] loss: 0.021\n",
      "[37,   300] loss: 0.013\n",
      "[38,   100] loss: 0.015\n",
      "[38,   200] loss: 0.012\n",
      "[38,   300] loss: 0.019\n",
      "[39,   100] loss: 0.015\n",
      "[39,   200] loss: 0.008\n",
      "[39,   300] loss: 0.012\n",
      "[40,   100] loss: 0.010\n",
      "[40,   200] loss: 0.012\n",
      "[40,   300] loss: 0.008\n",
      "[41,   100] loss: 0.023\n",
      "[41,   200] loss: 0.028\n",
      "[41,   300] loss: 0.022\n",
      "[42,   100] loss: 0.034\n",
      "[42,   200] loss: 0.030\n",
      "[42,   300] loss: 0.026\n",
      "[43,   100] loss: 0.024\n",
      "[43,   200] loss: 0.017\n",
      "[43,   300] loss: 0.027\n",
      "[44,   100] loss: 0.017\n",
      "[44,   200] loss: 0.022\n",
      "[44,   300] loss: 0.021\n",
      "[45,   100] loss: 0.027\n",
      "[45,   200] loss: 0.030\n",
      "[45,   300] loss: 0.048\n",
      "[46,   100] loss: 0.039\n",
      "[46,   200] loss: 0.036\n",
      "[46,   300] loss: 0.034\n",
      "[47,   100] loss: 0.028\n",
      "[47,   200] loss: 0.019\n",
      "[47,   300] loss: 0.032\n",
      "[48,   100] loss: 0.013\n",
      "[48,   200] loss: 0.013\n",
      "[48,   300] loss: 0.016\n",
      "[49,   100] loss: 0.006\n",
      "[49,   200] loss: 0.010\n",
      "[49,   300] loss: 0.007\n",
      "[50,   100] loss: 0.004\n",
      "[50,   200] loss: 0.001\n",
      "[50,   300] loss: 0.005\n",
      "[51,   100] loss: 0.002\n",
      "[51,   200] loss: 0.001\n",
      "[51,   300] loss: 0.000\n",
      "[52,   100] loss: 0.001\n",
      "[52,   200] loss: 0.002\n",
      "[52,   300] loss: 0.003\n",
      "[53,   100] loss: 0.001\n",
      "[53,   200] loss: 0.003\n",
      "[53,   300] loss: 0.001\n",
      "[54,   100] loss: 0.003\n",
      "[54,   200] loss: 0.001\n",
      "[54,   300] loss: 0.001\n",
      "[55,   100] loss: 0.002\n",
      "[55,   200] loss: 0.000\n",
      "[55,   300] loss: 0.000\n",
      "[56,   100] loss: 0.000\n",
      "[56,   200] loss: 0.002\n",
      "[56,   300] loss: 0.001\n",
      "[57,   100] loss: 0.000\n",
      "[57,   200] loss: 0.003\n",
      "[57,   300] loss: 0.000\n",
      "[58,   100] loss: 0.000\n",
      "[58,   200] loss: 0.003\n",
      "[58,   300] loss: 0.000\n",
      "[59,   100] loss: 0.000\n",
      "[59,   200] loss: 0.000\n",
      "[59,   300] loss: 0.003\n",
      "[60,   100] loss: 0.001\n",
      "[60,   200] loss: 0.001\n",
      "[60,   300] loss: 0.000\n",
      "[61,   100] loss: 0.000\n",
      "[61,   200] loss: 0.000\n",
      "[61,   300] loss: 0.001\n",
      "[62,   100] loss: 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      7\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mfor\u001b[39;00m i, (inputs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      9\u001b[0m         optimzer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "Cell \u001b[0;32mIn[110], line 9\u001b[0m, in \u001b[0;36mAmharicDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      7\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_filenames[idx]\n\u001b[1;32m      8\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir, filename))\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m image \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39;49mToTensor()(image)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Convert the image to grayscale using .convert('L')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(filename\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/transforms/functional.py:171\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    169\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Amann()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimzer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimzer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        # print(\"input shape\", inputs.shape, outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 85.51%\n"
     ]
    }
   ],
   "source": [
    "# pandas dataframe to load the csv map file\n",
    "df = pd.read_csv(\"../supported_chars.csv\")\n",
    "prop = FontProperties()\n",
    "prop.set_file(\"../Fonts/NotoSerif.ttf\")\n",
    "    \n",
    "\n",
    "#test model accuracy\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate over test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Pass input through model to get predictions\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Get predicted labels\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Update total count and correct count\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Visualize input with image viewer along with the prediction\n",
    "        # for i in range(inputs.size(0)):\n",
    "        #     image = inputs[i].numpy()\n",
    "        #     is_correct = predicted[i].item() == labels[i].item()\n",
    "            \n",
    "        #     label_str = \"Correct\" if is_correct else \"Mistaken\"\n",
    "        #     character = df[\"Character\"][labels[i].item()]\n",
    "        #     predicted_char = df[\"Character\"][predicted[i].item()]\n",
    "            \n",
    "        #     # Save test results\n",
    "        #     plt.imshow(np.squeeze(image), cmap='gray')\n",
    "        #     plt.title(f\"{label_str}: Actual -> {character}, predicted -> {predicted_char}\", fontproperties=prop)\n",
    "        #     plt.savefig(f\"../test_results/{label_str}_{i}_{correct}.png\")\n",
    "                \n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy on test set: {:.2f}%'.format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "torch.save(model.state_dict(), \"amann.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Warning:model is Amann(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear): Linear(in_features=576, out_features=34, bias=True)\n",
      "  (linear2): Linear(in_features=34, out_features=238, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "                        Proper storage of interactively declared classes (or instances\n",
      "                        of those classes) is not possible! Only instances\n",
      "                        of classes in real modules on file system can be %store'd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%store model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
